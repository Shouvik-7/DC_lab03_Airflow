[2023-11-19T20:43:34.693+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: outcomes_dag.extract2 manual__2023-11-19T20:43:32.929658+00:00 [queued]>
[2023-11-19T20:43:34.697+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: outcomes_dag.extract2 manual__2023-11-19T20:43:32.929658+00:00 [queued]>
[2023-11-19T20:43:34.697+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-11-19T20:43:34.707+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): extract2> on 2023-11-19 20:43:32.929658+00:00
[2023-11-19T20:43:34.711+0000] {standard_task_runner.py:57} INFO - Started process 66 to run task
[2023-11-19T20:43:34.719+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'outcomes_dag', 'extract2', 'manual__2023-11-19T20:43:32.929658+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/outcomes_dag.py', '--cfg-path', '/tmp/tmpzsovv1nm']
[2023-11-19T20:43:34.722+0000] {standard_task_runner.py:85} INFO - Job 5: Subtask extract2
[2023-11-19T20:43:34.776+0000] {task_command.py:416} INFO - Running <TaskInstance: outcomes_dag.extract2 manual__2023-11-19T20:43:32.929658+00:00 [running]> on host 549ad0bd2371
[2023-11-19T20:43:34.846+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='outcomes_dag' AIRFLOW_CTX_TASK_ID='extract2' AIRFLOW_CTX_EXECUTION_DATE='2023-11-19T20:43:32.929658+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-11-19T20:43:32.929658+00:00'
[2023-11-19T20:45:29.425+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/boto3/s3/transfer.py", line 292, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/tasks.py", line 139, in __call__
    return self._execute_main(kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/tasks.py", line 162, in _execute_main
    return_value = self._main(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/s3transfer/upload.py", line 764, in _main
    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/botocore/client.py", line 535, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/botocore/client.py", line 980, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (RequestTimeout) when calling the PutObject operation (reached max retries: 4): Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/etl_scripts/extract.py", line 15, in extract_data
    s3.Bucket('animalshelter').upload_file(Filename='shelter.csv',Key='shelter.csv')
  File "/home/airflow/.local/lib/python3.8/site-packages/boto3/s3/inject.py", line 233, in bucket_upload_file
    return self.meta.client.upload_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/boto3/s3/inject.py", line 143, in upload_file
    return transfer.upload_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/boto3/s3/transfer.py", line 298, in upload_file
    raise S3UploadFailedError(
boto3.exceptions.S3UploadFailedError: Failed to upload shelter.csv to animalshelter/shelter.csv: An error occurred (RequestTimeout) when calling the PutObject operation (reached max retries: 4): Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.
[2023-11-19T20:45:29.454+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=outcomes_dag, task_id=extract2, execution_date=20231119T204332, start_date=20231119T204334, end_date=20231119T204529
[2023-11-19T20:45:29.467+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 5 for task extract2 (Failed to upload shelter.csv to animalshelter/shelter.csv: An error occurred (RequestTimeout) when calling the PutObject operation (reached max retries: 4): Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.; 66)
[2023-11-19T20:45:29.481+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
